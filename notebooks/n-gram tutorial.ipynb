{
 "metadata": {
  "name": "",
  "signature": "sha256:88ca275159a47d9d00c1a73f03f72a074fc7c395fed3477320fc5608c6247e7a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Table of Contents"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. [Introduction](#Introduction) - trying to understand it\n",
      "2. [Corpora and Tokenization](#Corpora-and-Tokenization) - finding food for it\n",
      "3. [Coding the Model](#Coding-the-Model) - having a go at it\n",
      "4. [Learning from Reading](#Learning-from-Reading) - reading someone else's go at it\n",
      "5. [Uses](#Uses) - Where is it?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are probably a lot of mathematical expositions of the n-gram model available online or in textbooks. This introductory section approaches the problem from the other direction: explaining and implementing an n-gram model intuitively.\n",
      "\n",
      "If you're more interested in code, you might want to skip to the next section: [Corpora and Tokenization](#Corpora-and-Tokenization).\n",
      "\n",
      "Let's start with a question. Why do we have expectations about the words we read? That we have have these expectations is undeniable. Our first thought is that our expectations are based on the structure of the language -- the *syntax*. But we quickly think of many other things that give us expectations about the coming words: common idiom, the author's style, or even other knowledge we have that's external to the text can influence our expectations.\n",
      "\n",
      "If we want to be able to quantify and compute these expectations, we are not going to have access to a lot of that additional information. We might have some, though: collections of the author's texts, and maybe larger collections of that author's language in general, all lovingly digitized, give us a lot of relevant data. A human being with our data could probably read for two years and emerge with some extremely good expectations. How can we do the same thing with a computer?\n",
      "\n",
      "This is the point where I like to start thinking about everything as likelihoods. That's why I like the word \"expectation\" -- it's just a likelihood of an event, but we understand expectations. We know what an expectation feels like. It's a feeling like something is right for a spot -- it just ought to be, obviously. It fits. So carry that intuition over to the more technical word, *likelihood*. A likelihood is just an expectation of an event, quantified as a real number between 0 and 1. When we feel an expectation, we're unconsciously assigning a likelihood to an event -- \"I think this future event is x% likely, given this stuff that's already known.\" Or something. Who really knows, honestly. But let's pretend that's what we're doing, because that's what we know how to program computers to do.\n",
      "\n",
      "Anyway, we're left with trying to calculate \"this future event ix x% likely given this known past stuff\". In order to calculate a likelihood like that, we can see the \"known past stuff\" happen a bunch of times, and count how many times it's followed by the future event in question. Or we can use the Bayes Theorem, but we don't need to get into that. For our purposes, just looking and counting is fine.\n",
      "\n",
      "Of course, we are still left wondering how to make this \"known past stuff\" and \"future event\" explicit for our program. In a sense, this is the heart of the NLP problem. The n-gram model is just one way of thinking about \"past stuff\" and \"future events\", and it's based on the idea that language itself usually has some fairly obvious content delimeters. Typically, the contents of each of the delimited blobs are very tightly coupled syntactically, logically, and/or stylistically. \n",
      "\n",
      "In English, the sentence is the obvious example. It's really hard to predict the first word of a sentence, compared to how easy it is to predict the further words, even if you know the words in the sentence beforehand. There's a less meaningful attachment between words in two separate sentences. So we recognize that sentence boundaries are important. The other, purely practical, advantage of the sentence-sized blob is that we have a lot of them about. Paragraphs are also blobby, but they are much more uncommon, which makes them harder to use.\n",
      "\n",
      "Another great blob in the English language is the \"word\". Although words are not necessarily the \"smallest unit of meaning\" (thanks to prefixes, suffixes, and compound words), they seem like as good a \"unit of meaning\" as anything we can find in natural language. There are some models that try to consider sub-word-level meaning, but we aren't covering that here (yet). \n",
      "\n",
      "Since a word is a small, fairly predictable thing, and sentences provide such convenient content delimiters, the n-gram model is focused on estimating the likelihood of the *next word in the sentence* -- that's our future event -- given *the sentence so far* as our past data.\n",
      "\n",
      "I tried to emphasize that this is a fairly arbitrary distinction. There is nothing essential about using words and sentences as the most meaningful units when analyzing natural language. It just seems to make the most sense to us right now."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unfortunately, it turns out that even calculating the *likelihood of the next word in the sentence, given the sentence so far* is not very practical. Long sentences might throw a wrench into your counting process (remember, that's how we're calculating the likelihoods for the next word -- by counting all previous times we've seen the sentence so far) by being utterly unique, which would make it impossible for you predict the next word, although you would never encounter one in a well-written notebook like this one! So, it turns out we actually have to throw out most of the sentence. It's a matter of data scarcity making results seem overly certain.\n",
      "\n",
      "When we have a huge collection of text -- millions of words -- then although many sentences are still repeated only once or twice, 3-4 word segments almost always appear multiple times. And that's important, because we need many appearances for our model to make sane predictions. To see why, you need to remember that the model doesn't know about any word combinations it hasn't seen before. Imagine rolling a dice twice and it comes up 1 and 2. Now this is your data about the dice.\n",
      "\n",
      "1. 50%\n",
      "2. 50%\n",
      "3. 0%\n",
      "4. 0%\n",
      "5. 0%\n",
      "6. 0%\n",
      "\n",
      "Imagine this is all the data you have about the dice. You don't even know what it's shaped like! So you just use the data for your prediction -- you have no choice, and really, no reason to expect anything other than a perfect prediction of future behavior, except for some vague misgivings about sample size. But, those percentages are *completely wrong* for a 6-sided dice, so the whole table is wrong, and you won't predict future behavior well at all! You need more data to make accurate guesses; two data points is not enough for the 6 possible outcomes.\n",
      "\n",
      "Actually, a human probably wouldn't just take that table at face value. They would figure, \"there's no way the others are 0%; we probably don't have enough data,\" because they are human and therefore scientific. So, they adjust the values before using their results. This kind of manual tinkering with probabilities is quintessentially human, but we often add similar functionality to our n-gram models so they don't choke on unexpected combinations, but just guess the probabilities. This is called fuzzing our model, and it can significantly improve results."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Corpora and Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A corpus is just a big collection of documents which are analyzed by the model program. Selection of a corpus is essential in real-world NLP. The corpus should be not too big, or the model may take too long to train, but it also can\u2019t be too small because the model needs a lot of text in order to learn. The more complex and varied the subject matter, the larger the corpus required. \n",
      "\n",
      "For this example, I will be using the gutenberg corpus from NLTK\u2019s collection of corpora. This is just a collection of the text from a bunch of public domain books, taken from [Project Gutenberg](http://www.gutenberg.org/). [NLTK](http://www.nltk.org/) stands for Natural Language Toolkit. It's a vast Python library that has a lot of utilities for dealing with natural language."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "!python -m nltk.downloader gutenberg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package gutenberg to /home/luke/nltk_data...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data]   Package gutenberg is already up-to-date!\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check that it installed correctly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import gutenberg\n",
      "gutenberg.raw('carroll-alice.txt')[1:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "\"Alice's Adventures in Wonderland by Lewis Carroll\""
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A raw-string representation of a book is not very meaningful to our model. The mathematics behind the n-gram model are described in terms of \"tokens\". For our purposes we can think of tokens as individual semantic units, like words or punctuation, that we want to consider in the model.\n",
      "\n",
      "This is important: tokens are not meant to represent the *syntax* -- physical structure -- of the text. They instead provide a consistent way to represent the *semantics* -- the meaning -- behind the text, in a way that our NLP model can easily parse.\n",
      "\n",
      "The process of breaking a character string into meaningful chunks is called *tokenization* in NLP circles, but many programmers also know it as *lexical analysis*, because that's what it's called when you are talking about programming languages. An NLP person might say, \"we want to turn this string of characters into a list of *tokens*\". A compiler guy might call them *atoms*. One vocabulary was created for describing natural language stuff, the other was made for computer language stuff, but they mean the same thing. I'll be using the natural language vocabulary, so you get used to it.\n",
      "\n",
      "NTLK has [a number of tokenizers](http://www.nltk.org/api/nltk.tokenize.html) for us to use, as we've come to expect. In your projects, you should probably use these. For instance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "word_tokenize(sent_tokenize(gutenberg.raw('carroll-alice.txt'))[1])[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['Down',\n",
        " 'the',\n",
        " 'Rabbit-Hole',\n",
        " 'Alice',\n",
        " 'was',\n",
        " 'beginning',\n",
        " 'to',\n",
        " 'get',\n",
        " 'very',\n",
        " 'tired']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this tutorial, though, we're going to make our own tokenizer: a nice, simple one that we can modify easily. Our tokenizer is going to be primarily powered by regular expressions. Yes, those horrid things, which are notoriously unable to parse anything defined in a recursive way, are going to be applied to the impossibly complex grammar of English text! \n",
      "\n",
      "Since our tokenizer will be using hardcoded regex substitutions as a tokenization strategy, we have to make choices about what changes we want to make to the source text. Remember, the choices we make here are important because of how they affect the n-gram model. Having said that, there are no wrong answers: different tokenization schemes will work better or worse depending on your corpus and your model and your goals.\n",
      "\n",
      "To start with, we want to do some basic transformations:\n",
      "\n",
      "1. Standardize whitespace\n",
      "2. Split the text on spaces so we have a list of tokens\n",
      "3. CAPSLOCK all tokens (so `Splice` and `splice` are the same -- important for our model)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(words):\n",
      "    re.sub(r'[-\\n]', \" \", words) # standardize whitespace\n",
      "    return [w.upper() for w in words.split(' ') if w != \"\"]\n",
      "\n",
      "tokenize(gutenberg.raw('carroll-alice.txt')[1:50])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[\"ALICE'S\", 'ADVENTURES', 'IN', 'WONDERLAND', 'BY', 'LEWIS', 'CARROLL']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great. What else should our tokenizer be doing? \n",
      "\n",
      "For starters, there's a lot of punctuation we should deal with, both useful (`.,:;!?`) and not so useful (`()[]{}#$%`). We have to decide what punctuation we want to preserve as meaningful tokens, and what we can discard as simply distracting.\n",
      "\n",
      "Obviously, sentence-breaking punctuation is important. Our model's every generated sentence is going to begin and end with a sentence break, so we need a consistent way to represent them. Although we could differentiate between `!`, `?`, and `.`, we can get more meaningful results with such a small corpus by replacing all of these with a consistent `[SENTENCE-BREAK]` token.\n",
      "\n",
      "Other common punctuation is also important if we want our model to capture any sense of grammar. At least we can preserve `,`, `;`, `:`, and `...`. Parenthesis are a little meaningful as well since an open-parenthesis indicates the start of a new clause in the sentence (a parenthetical clause). But for this simple tokenizer, I want to leave them out.\n",
      "\n",
      "Symbols like `\\/#%$^*@` (just hold Shift and spam your number keys for examples) are also a tiny bit meaningful, but not enough to keep them. This is especially true because the symbols are fairly rare, so in such a small corpus, our model won't see enough repetions of the symbol to \"understand\" them statistically. Braces `[]{}` are in the same boat, and although quotation marks `\"` are quite common, their meaningfulness is questionable, and the n-gram model has no sense of \"matching\" quotes or any other elements that come in pairs, so we will get rid of them.\n",
      "\n",
      "On the topic of symbolic rarity, *numbers* are notoriously annoying for n-gram models or any statistical models on text. Our model treats each token as having unique meaning, and while 122 is certainly different from 123, it's very hard to find useful statistical information in that distinction. If each number is treated as discrete, we can end up with tens or hundreds of numeric tokens with only one or two priors, which is bad news. It's pretty much impossible for the n-gram model to extract meaning from such sparse data. \n",
      "\n",
      "On the other hand, the *idea* of numbers is useful -- the idea that, hey, a number should go in this spot. The exact number doesn't really matter, but there definitely should be a number here. We will represent this by replacing every number with a token, like `[NUMBER]`, and assuming that the fact that there *is* a number is more important than the actual digits.\n",
      "\n",
      "Let's put these ideas into code. Once again, remember that you can, and should, fiddle with these in your own project, or use a preexisting tokenizer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from functools import reduce\n",
      "\n",
      "def tokenize(ws):\n",
      "    replacements = [\n",
      "         [\"[-\\n]\",                  \" \"] # Hyphens to whitespace\n",
      "        ,[r'[][(){}#$%\"]',          \"\"] # Strip unwanted characters\n",
      "        ,[r'\\s([./-]?\\d+)+[./-]?\\s',\" [NUMBER] \"] # Standardize numbers\n",
      "        ,[r'\\.{3,}',                \" [ELLIPSIS] \"]\n",
      "        ,[r',',                     \" [COMMA] \"]\n",
      "        ,[r';',                     \" [SEMICOLON] \"]\n",
      "        ,[r':',                     \" [COLON] \"]\n",
      "        ,[r'[.!?]',                 \" [SENTENCE-BREAK] \"]\n",
      "    ]\n",
      "    resub = lambda ws, repls: re.sub(repls[0], repls[1], ws)\n",
      "    tokens = [w.upper() for w in \n",
      "              reduce(resub, replacements, ws).split(' ') if w != '']\n",
      "    return tokens + [\"[SENTENCE-BREAK]\"]\n",
      "\n",
      "tokenize(\"This is 1 line of Text? How, does, it look; I wonder...\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "['THIS',\n",
        " 'IS',\n",
        " '[NUMBER]',\n",
        " 'LINE',\n",
        " 'OF',\n",
        " 'TEXT',\n",
        " '[SENTENCE-BREAK]',\n",
        " 'HOW',\n",
        " '[COMMA]',\n",
        " 'DOES',\n",
        " '[COMMA]',\n",
        " 'IT',\n",
        " 'LOOK',\n",
        " '[SEMICOLON]',\n",
        " 'I',\n",
        " 'WONDER',\n",
        " '[ELLIPSIS]',\n",
        " '[SENTENCE-BREAK]']"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A few notes about this new code. It defines a table of `[regex, replacement]` pairs and then applies each one in turn to the passed-in text `ws`. It's important that the application is in-order, or the sentence-break replacement might clobber the ellipsis replacement (for instance).\n",
      "\n",
      "We do this with the `reduce` function, which is typically hard to grok at first. The way it's used here, it applies the `resub` function to each `[regex, replacement]` in `replacements` in turn. `resub` returns the modified text and the search begins again with the next regex. In the end, the `reduce` call returns the value returned by the last call of `resub`, which is the fully-modified text.\n",
      "\n",
      "After the regexes are applied, the text is split on spaces and each token is capslocked.\n",
      "\n",
      "Finally, we append `[SENTENCE-BREAK]` to the end. This will prevent our model from \"walking off\" the end of the corpus if it ends midsentence. (More concretely: if the corpus ends in some word $w_L$, and $w_L$ doesn't appear anywhere else in the corpus, then there won't be a word $w_{L+1}$ such that $P(w_{L+1}|w_L) \\neq 0$. This is only acceptable for `[SENTENCE-BREAK]`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Coding the Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implementing n-gram model training in Python is fairly straightforward. I am going to use Python's `defaultdict`, because I think it helps the model's logic shine by automatically instantiating new dict keys. \n",
      "\n",
      "At its heart, the model is a conditional frequency distribution, modeled as a nested dictionary. The keys of the outer dictionaries are the priors, which in Python are represented as a tuple of length $n-1$ for an $n$-gram model. The keys of the inner dictionaries are single words, and their values are frequencies. For example, in a $3$-gram model,\n",
      "\n",
      "`model[(\"four\", \"score\")][\"and\"] = ` $F(\\text{and}|\\text{four score})$\n",
      "\n",
      "Note that we don't have to convert the frequency into a probability (i.e. normalize it) but we could if we wanted"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "from functools import partial\n",
      "from itertools import dropwhile\n",
      "\n",
      "def make_model(n, words):\n",
      "    prior_n = n-1 # n-1 words in the prior tuple\n",
      "    freq_dist = partial(defaultdict, int) # frequency distribution constructor\n",
      "    model = defaultdict(freq_dist)\n",
      "\n",
      "    for index in range(prior_n,len(words)):\n",
      "        prior = words[index-prior_n:index]\n",
      "        if \"[SENTENCE-BREAK]\" in prior:\n",
      "            # Discard unneeded context\n",
      "            prior = dropwhile(lambda x: x != \"[SENTENCE-BREAK]\", prior)\n",
      "        # Note: tuples are hashable\n",
      "        model[tuple(prior)][words[index]] += 1\n",
      "\n",
      "    return model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead of using `defaultdict`, we can also use the NLTK's built-in `ConditionalFreqDist` class (which inherits from `defaultdict`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.probability import ConditionalFreqDist\n",
      "\n",
      "def make_model(n, words):\n",
      "    prior_n = n-1 # n-1 words in the prior tuple\n",
      "    model = ConditionalFreqDist()\n",
      "\n",
      "    for index in range(prior_n,len(words)):\n",
      "        prior = words[index-prior_n:index]\n",
      "        if \"[SENTENCE-BREAK]\" in prior:\n",
      "            # Discard unneeded context\n",
      "            prior = dropwhile(lambda x: x != \"[SENTENCE-BREAK]\", prior)\n",
      "        # Note: tuples are hashable\n",
      "        model[tuple(prior)].inc(words[index])\n",
      "\n",
      "    return model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we want to be really conise (and don't mind a slight change in semantics), we could go with this opaque but fun one-liner:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.probability import ConditionalFreqDist as cfr\n",
      "\n",
      "def make_model(n, ws):\n",
      "    return cfr((tuple(ws[i-n+1:i]), ws[i]) for i in range(n-1,len(ws)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course we also have the option of using the more fully-featured n-gram model built in to NLTK."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.model.ngram import NgramModel\n",
      "\n",
      "model = NgramModel(2, tokenize(\"This is a sentence. So is this.\"))\n",
      "model.choose_random_word([\"this\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "'SENTENCE'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you mess around with `NgramModel`, you'll notice some interesting behavior. How can it estimate the probabilities of words that it hasn't seen? (If you actually read the Introduction, you might already know the answer to this question -- it's called fuzzing)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning from Reading"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take a look at [the source](http://www.nltk.org/_modules/nltk/model/ngram.html#NgramModel) for the NgramModel class. What does a mature implementation have that our ten-line routine lacks? Is there a more efficient way to implement the model in code? These are important questions; you can learn a lot by examining code in NLTK or other big projects.\n",
      "\n",
      "Here's the code for creating the NgramModel class that NLTK uses:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NgramModel(ModelI):\n",
      "    def __init__(self, n, train, pad_left=True, pad_right=False,\n",
      "                 estimator=None, *estimator_args, **estimator_kwargs):\n",
      "        self._n = n\n",
      "        self._lpad = ('',) * (n - 1) if pad_left else ()\n",
      "        self._rpad = ('',) * (n - 1) if pad_right else ()\n",
      "\n",
      "        if estimator is None:\n",
      "            estimator = _estimator\n",
      "\n",
      "        cfd = ConditionalFreqDist()\n",
      "        self._ngrams = set()\n",
      "\n",
      "\n",
      "        # If given a list of strings instead of a list of lists, create enclosing list\n",
      "        if (train is not None) and isinstance(train[0], compat.string_types):\n",
      "            train = [train]\n",
      "\n",
      "        for sent in train:\n",
      "            for ngram in ngrams(chain(self._lpad, sent, self._rpad), n):\n",
      "                self._ngrams.add(ngram)\n",
      "                context = tuple(ngram[:-1])\n",
      "                token = ngram[-1]\n",
      "                cfd[context].inc(token)\n",
      "\n",
      "        if not estimator_args and not estimator_kwargs:\n",
      "            self._model = ConditionalProbDist(cfd, estimator, len(cfd))\n",
      "        else:\n",
      "            self._model = ConditionalProbDist(cfd, estimator, *estimator_args, **estimator_kwargs)\n",
      "\n",
      "        # recursively construct the lower-order models\n",
      "        if n > 1:\n",
      "            self._backoff = NgramModel(n-1, train, pad_left, pad_right,\n",
      "                                       estimator, *estimator_args, **estimator_kwargs)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see what's going on. \n",
      "\n",
      "The `__init__` function populates two main data structures: `cfd` is the now-familiar `ConditionalFrequencyDistribution`, and `_ngrams` is a set of $n$-length tuples which is kept for easily testing if a given ngram tuple has been seen in the training set.\n",
      "\n",
      "The `estimator` is a way of constructing probability distributions from frequency distributions. We'll get into its use later also.\n",
      "\n",
      "This model is designed to work on `list(list(string))`s, which typically means a list of sentences which are lists of tokens or words. This can be beneficial because you don't have to worry about \"walking off\" sentences or being strict with your `[SENTENCE-BREAK]` tokens. \n",
      "\n",
      "If you pass in a `list(string)`, it's wrapped in another list, but not split into sentences. The model makes no assumptions about the meaning of its training data -- this same model could be used for Japanese as well as English, just remember that the text is read in the opposite direction. Tokenization is left completely to the calling code.\n",
      "\n",
      "Finally we reach the heart of the training process."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for sent in train:\n",
      "    for ngram in ngrams(chain(self._lpad, sent, self._rpad), n):\n",
      "        self._ngrams.add(ngram)\n",
      "        context = tuple(ngram[:-1])\n",
      "        token = ngram[-1]\n",
      "        cfd[context].inc(token)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ah, now this is starting to look familiar, especially the last three lines.\n",
      "\n",
      "Each sentence in the training set is padded up. The `_lpad` and `_rpad`s are optional, but `_lpad` is enabled by default so the first word in the sentence can have a context, even if the context is just a bunch of placeholder strings.\n",
      "\n",
      "Each ngram tuple $(w_{(i-n+1)}, w_{(i-n+2)}, \\dots, w_i)$ is stored in `_ngrams`, and then split into the context (aka the prior) $(w_{(i-n+1)}, w_{(i-n+2)}, \\dots, w_{(i-1)})$ and the current token $w_i$, and put into the `cfd`.\n",
      "\n",
      "There's something important here that's easy to miss: we're outsourcing the actual ngram production to `ngrams()`, because it's useful in other areas. This function is defined in `nltk.util`. Let's look at it too. But before we do, let's think about what we expect this function to be doing. It's being passed a `list(string)` and an $n$, and whatever it outputs is being added straight to the `_ngrams` set. This tells us it must return an iterable over ngram tuples.\n",
      "\n",
      "With that in mind, how would you implement `ngrams()`? I would probably do something like this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _ngrams(words, n):\n",
      "    return (tuple(words[i-(n-1):i+1]) for i in range(n-1, len(words)))\n",
      "\n",
      "list(_ngrams(tokenize(\"This is a sentence\"), 3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "[('THIS', 'IS', 'A'),\n",
        " ('IS', 'A', 'SENTENCE'),\n",
        " ('A', 'SENTENCE', '[SENTENCE-BREAK]')]"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alright, let's compare and contrast with the [NTLK implementation][0], duped below. Once again, I stripped block comments.\n",
      "\n",
      "[0]: http://www.nltk.org/_modules/nltk/util.html#ngrams"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ngrams(sequence, n, pad_left=False, pad_right=False, pad_symbol=None):\n",
      "\n",
      "    sequence = iter(sequence)\n",
      "    if pad_left:\n",
      "        sequence = chain((pad_symbol,) * (n-1), sequence)\n",
      "    if pad_right:\n",
      "        sequence = chain(sequence, (pad_symbol,) * (n-1))\n",
      "\n",
      "    history = []\n",
      "    while n > 1:\n",
      "        history.append(next(sequence))\n",
      "        n -= 1\n",
      "    for item in sequence:\n",
      "        history.append(item)\n",
      "        yield tuple(history)\n",
      "        del history[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, since the function is used elsewhere, there is redundant padding functionality. Our NgramModel passes in padded strings already, so that capacity is not used. But what about the rest of it?\n",
      "\n",
      "There's actually something really interesting going on here. This is a [generator function](https://wiki.python.org/moin/Generators), which are one of my favorite Python features. If you're not familiar with it, read the wiki, because I'm having trouble explaining it briefly.\n",
      "\n",
      "So, are you back? Now that we understand generator functions, we know that `yield` is a keyword that is similar to `return` -- it's part of the function's output. What are we `yield`ing here? Well, the same list -- `history` -- over and over again. To see why, let's try running this function to generate some quadrigrams:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.util import ngrams\n",
      "list(ngrams(tokenize(\"This is a slightly longer sentence\"), 4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[('THIS', 'IS', 'A', 'SLIGHTLY'),\n",
        " ('IS', 'A', 'SLIGHTLY', 'LONGER'),\n",
        " ('A', 'SLIGHTLY', 'LONGER', 'SENTENCE'),\n",
        " ('SLIGHTLY', 'LONGER', 'SENTENCE', '[SENTENCE-BREAK]')]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There's a lot of repetition there. The ngram is something like a \"moving window\" into the text. It makes sense from a performance perspective to preserve the elements that are repeated between grams, dropping from the beginning and appending to the end.\n",
      "\n",
      "So this function first produces a list of the first $n$ words, and then for each next word in the sequence, it `del`s the \"oldest\" word in `history` and appends the new one to produce the next ngram.\n",
      "\n",
      "This might involve less memory allocation since it only uses a single list. But it does still have to create a new tuple for each ngram (you can only hash immutable data types), so I'm not sure if the savings are significant compared to our naive `_ngrams` implementation. Let's run a quick test:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "token_poetry = tokenize(gutenberg.raw(\"austen-emma.txt\"))\n",
      "%timeit -n 5 list(ngrams(token_poetry, 4))\n",
      "%memit list(ngrams(token_poetry, 4))\n",
      "%timeit -n 5 list(_ngrams(token_poetry, 4))\n",
      "%memit list(_ngrams(token_poetry, 4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5 loops, best of 3: 115 ms per loop\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "peak memory: 79.31 MiB, increment: 11.40 MiB\n",
        "5 loops, best of 3: 151 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "peak memory: 79.29 MiB, increment: 11.64 MiB\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Seems using an inplace list is a little faster than the naive approach, but not much. \n",
      "\n",
      "Note that generator functions are lazily evaluated, so we force evaluation by pulling the iterable into an actual list. (Our function `_ngrams` also returns an iterator because it uses a [generator expression](https://docs.python.org/3/reference/expressions.html#generator-expressions), so we have to give it the same treatment).\n",
      "\n",
      "Anyway. Enough comparison.\n",
      "\n",
      "Now we know how `ngram()` works, let's carry on with `NgramModel.__init__()`. The next lines are:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not estimator_args and not estimator_kwargs:\n",
      "    self._model = ConditionalProbDist(cfd, estimator, len(cfd))\n",
      "else:\n",
      "    self._model = ConditionalProbDist(cfd, estimator, *estimator_args, **estimator_kwargs)\n",
      "\n",
      "# recursively construct the lower-order models\n",
      "if n > 1:\n",
      "    self._backoff = NgramModel(n-1, train, pad_left, pad_right,\n",
      "                               estimator, *estimator_args, **estimator_kwargs)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, once again we have some `estimator` business. \n",
      "\n",
      "Then, we are actually doing a *recursive construction* of all n-gram models with a lower $n$, and storing it in `_backoff`. This seems like a potential performance concern! Why do we need so many models?\n",
      "\n",
      "Surprisingly, this is a pretty common tactic. As $n$ grows, the $n$-gram model retains more and more context. This can be beneficial -- certainly, it gets us closer to the truth, which is that each word is contextualized by the *entire* preceding text and arguably the succeeding text as well. But it can also be dangerous: as we add more and more context, the *incidence* of a specific context decreases. For instance, consider quintigrams: it's pretty unlikely that any but the biggest corpus is going to contain many quintigrams you throw at it, and sexigrams are even worse.\n",
      "\n",
      "So what do you do if you can't match the provided context exactly? Just start throwing context out until you find a match -- which is what the $n-1$ model in `_backoff` is for.\n",
      "\n",
      "Concretely: what if you are trying to find the probability of \"steamed\" following \"I get all\" $P(\\text{steamed}|\\text{I get all})$, but your corpus doesn't contain nursery rhymes? You can \"back off\" to looking for $P(\\text{steamed}|\\text{get all})$. And if you can't find that, back off to $P(\\text{steamed}|\\text{all})$, which maybe exists in your corpus because it was found in \"They all steamed vegetables every morning\" or \"her clothes were all steamed and pressed\". This \"backing off\" the context can give you an answer, but at the cost of discarding information."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Uses"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from encodings import base64_codec\n",
      "base64_codec.base64_decode(b\"VGhlcmUncyBub3RoaW5nIGhlcmUuLi4=\")[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "b\"There's nothing here...\""
       ]
      }
     ],
     "prompt_number": 37
    }
   ],
   "metadata": {}
  }
 ]
}