{
 "metadata": {
  "name": "",
  "signature": "sha256:96580eb849fcfdf8f3cc69efa4c8591ef905af09699bff1fb745f5d39deac9f4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preparing the Corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Selection of a corpus is essential in real-world NLP. The corpus should be not too big, or the model may take too long to train, but it also can\u2019t be too small (intuitively, the model needs a lot of text in order to learn. The more complex and varied the subject matter, the larger the corpus required.) For this example, I will be using the gutenberg corpus from NLTK\u2019s collection of corpora. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "!python -m nltk.downloader gutenberg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package gutenberg to /home/luke/nltk_data...\r\n",
        "[nltk_data]   Package gutenberg is already up-to-date!\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check that it installed correctly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import gutenberg\n",
      "gutenberg.raw('carroll-alice.txt')[1:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "\"Alice's Adventures in Wonderland by Lewis Carroll\""
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A raw-string representation of a book is not very meaningful to our model. We need to split the text into tokens. For our purposes we can think of tokens as \"semantic units\", like words or punctuation, that we want to consider in the model.\n",
      "\n",
      "The process of breaking a character string into meaningful chunks is called *tokenization* in NLP circles, but many programmers also know it as *lexical analysis*. An NLP person might say, \"we want to turn this string of characters into a list of *tokens*\". A compiler guy might call them *atoms*. Same thing.\n",
      "\n",
      "NTLK has [a number of tokenizers](http://www.nltk.org/api/nltk.tokenize.html) for us to use. For instance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import sent_tokenize,word_tokenize\n",
      "word_tokenize(sent_tokenize(gutenberg.raw('carroll-alice.txt'))[1])[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "['Down',\n",
        " 'the',\n",
        " 'Rabbit-Hole',\n",
        " 'Alice',\n",
        " 'was',\n",
        " 'beginning',\n",
        " 'to',\n",
        " 'get',\n",
        " 'very',\n",
        " 'tired']"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We're going to make our own tokenizer though, a nice simple one we can modify later. Our tokenizer is going to be primarily regex-powered. Since it is using hardcoded rules, we have to make choices about what changes we want to make to the source text so that our model can better grasp the semantics.\n",
      "\n",
      "To start with, we want to:\n",
      "\n",
      "1. Standardize whitespace\n",
      "2. Split the text on spaces\n",
      "3. CAPSLOCK all words (so `Splice` and `splice` are made identical)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(ws):\n",
      "    re.sub(r'[-\\n]', \" \", ws) # standardize whitespac\n",
      "    return [w.upper() for w in ws.split(' ') if w != \"\"]\n",
      "\n",
      "tokenize(gutenberg.raw('carroll-alice.txt')[1:50])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[\"ALICE'S\", 'ADVENTURES', 'IN', 'WONDERLAND', 'BY', 'LEWIS', 'CARROLL']"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What else should our tokenizer be doing? \n",
      "\n",
      "For starters, there's a lot of punctuation, both useful `(.,:;!?` and not so useful `)[]{}#$%`. We have to decide what punctuation we want to preserve as meaningful tokens, and what we can discard as simply distracting.\n",
      "\n",
      "Obviously, sentence-breaking punctuation is important. Our model's every generated sentence is going to begin and end with a sentence break, so we need a consistent way to represent them. Although we could differentiate between `!`, `?`, and `.`, we can get more meaningful results with such a small corpus by replacing all of these with a consistent `[SENTENCE-BREAK]` token.\n",
      "\n",
      "Other common punctuation is also important if we want our model to capture any sense of grammar. At least we can preserve `,`, `;`, `:`, and `...`. Parenthesis are a little meaningful as well since an open-parenthesis indicates the start of a clause. But for this simple tokenizer, I want to leave them out.\n",
      "\n",
      "Symbols like `\\/#%$^*@` (just hold Shift and spam your number keys for examples) are also a tiny bit meaningful, but not enough to keep them. This is especially true because the symbols are fairly rare, so in such a small corpus, our model won't have time to grok them. Braces `[]{}` are in the same boat, and so are quotation marks `\"`, although they're far more common.\n",
      "\n",
      "On the topic of symbolic rarity, *numbers* are notoriously annyoing for n-gram models. Our model treats 123 completely separately from 122. Since it's rare to have only a few numbers repeated in a book, we can end up with tens or hundreds of numeric tokens with only one or two priors, which is bad news, because it's hard to extract meaning from such sparse data. On the other hand, the *idea* of numbers is useful. One solution here is to replace every number with a token, like `[NUMBER]`, and assume that the fact that there's a number is more important than what that number actually is.\n",
      "\n",
      "Let's put these ideas into code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from functools import reduce\n",
      "\n",
      "def tokenize(ws):\n",
      "    replacements = [\n",
      "         [\"[-\\n]\",                  \" \"] # Hyphens to whitespace\n",
      "        ,[r'[][(){}#$%\"]',          \"\"] # Strip unwanted characters\n",
      "        ,[r'\\s([./-]?\\d+)+[./-]?\\s',\" [NUMBER] \"] # Standardize numbers\n",
      "        ,[r'\\.{3,}',                \" [ELLIPSIS] \"]\n",
      "        ,[r',',                     \" [COMMA] \"]\n",
      "        ,[r';',                     \" [SEMICOLON] \"]\n",
      "        ,[r':',                     \" [COLON] \"]\n",
      "        ,[r'[.!?]',                 \" [SENTENCE-BREAK] \"]\n",
      "    ]\n",
      "    resub = lambda ws,repls: re.sub(repls[0], repls[1], ws)\n",
      "    tokens = [w.upper() for w in \n",
      "              reduce(resub, replacements, ws).split(' ') if w != '']\n",
      "    return tokens + [\"[SENTENCE-BREAK]\"]\n",
      "\n",
      "tokenize(\"This is 1 line of Text? How, does, it look; I wonder...\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['THIS',\n",
        " 'IS',\n",
        " '[NUMBER]',\n",
        " 'LINE',\n",
        " 'OF',\n",
        " 'TEXT',\n",
        " '[SENTENCE-BREAK]',\n",
        " 'HOW',\n",
        " '[COMMA]',\n",
        " 'DOES',\n",
        " '[COMMA]',\n",
        " 'IT',\n",
        " 'LOOK',\n",
        " '[SEMICOLON]',\n",
        " 'I',\n",
        " 'WONDER',\n",
        " '[ELLIPSIS]',\n",
        " '[SENTENCE-BREAK]']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A few notes about this new code. It defines a table of `[regex, replacement]` pairs and then applies each one in turn to the passed-in text `ws`. It's important that the application is in-order, or the sentence-break replacement might clobber the ellipsis replacement (for instance).\n",
      "\n",
      "We do this with the `reduce` function, which is typically hard to grok at first. The way it's used here, it applies the `resub` function to each `[regex, replacement]` in `replacements` in turn. `resub` returns the modified text and the search begins again with the next regex. In the end, the `reduce` call returns the value returned by the last call of `resub`, which is the fully-modified text.\n",
      "\n",
      "After the regexes are applied, the text is split on spaces and each token is capslocked.\n",
      "\n",
      "Finally, we append `[SENTENCE-BREAK]` to the end. This will prevent our model from \"walking off\" the end of the corpus if it ends midsentence. (More concretely: if the corpus ends in some word $w_L$, and $w_L$ doesn't appear anywhere else in the corpus, then there won't be a word $w_{L+1}$ such that $P(w_{L+1}|w_L) \\neq 0$. This is only acceptable for `[SENTENCE-BREAK]`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Coding the Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implementing n-gram model training in Python is fairly straightforward. I am going to use Python's `defaultdict`, because I think it helps the model's logic shine by automatically instantiating new dict keys. \n",
      "\n",
      "At its heart, the model is a conditional frequency distribution, modeled as a nested dictionary. The keys of the outer dictionaries are the priors, which in Python are represented as a tuple of length $n-1$ for an $n$-gram model. The keys of the inner dictionaries are single words, and their values are frequencies. For example, in a $3$-gram model,\n",
      "\n",
      "`model[(\"four\", \"score\")][\"and\"] = ` $F(\\text{and}|\\text{four score})$\n",
      "\n",
      "Note that we don't have to convert the frequency into a probability, but we could if we wanted"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "from functools import partial\n",
      "from itertools import dropwhile\n",
      "\n",
      "def make_model(n, words):\n",
      "    prior_n = n-1 # n-1 words in the prior tuple\n",
      "    freq_dist = partial(defaultdict, int) # frequency distribution constructor\n",
      "    model = defaultdict(freq_dist)\n",
      "\n",
      "    for index in range(prior_n,len(words)):\n",
      "        prior = words[index-prior_n:index]\n",
      "        if \"[SENTENCE-BREAK]\" in prior:\n",
      "            # Discard unneeded context\n",
      "            prior = dropwhile(lambda x: x != \"[SENTENCE-BREAK]\", prior)\n",
      "        # Note: tuples are hashable\n",
      "        model[tuple(prior)][words[index]] += 1\n",
      "\n",
      "    return model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead of using `defaultdict`, we can also use the NLTK's built-in `ConditionalFreqDist` class (which inherits from `defaultdict`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.probability import ConditionalFreqDist\n",
      "\n",
      "def make_model(n, words):\n",
      "    prior_n = n-1 # n-1 words in the prior tuple\n",
      "    model = ConditionalFreqDist()\n",
      "\n",
      "    for index in range(prior_n,len(words)):\n",
      "        prior = words[index-prior_n:index]\n",
      "        if \"[SENTENCE-BREAK]\" in prior:\n",
      "            # Discard unneeded context\n",
      "            prior = dropwhile(lambda x: x != \"[SENTENCE-BREAK]\", prior)\n",
      "        # Note: tuples are hashable\n",
      "        model[tuple(prior)].inc(words[index])\n",
      "\n",
      "    return model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we want to be really conise (and don't mind a slight change in semantics), we could go with this opaque but fun one-liner:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.probability import ConditionalFreqDist as cfr\n",
      "\n",
      "def make_model(n, ws):\n",
      "    return cfr((tuple(ws[i-n+1:i]), ws[i]) for i in range(n-1,len(ws)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course we also have the option of using the more fully-featured n-gram model built in to NLTK."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.model.ngram import NgramModel\n",
      "\n",
      "model = NgramModel(2, tokenize(\"This is a sentence. So is this.\"))\n",
      "model.choose_random_word([\"this\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "'SENTENCE'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you mess around with `NgramModel`, you'll notice some interesting behavior. How can it estimate the probabilities of words that it hasn't seen? \n",
      "\n",
      "Let's take a look at [the source](http://www.nltk.org/_modules/nltk/model/ngram.html#NgramModel) for the NgramModel class. Let's see what insights we can glean from a more mature implementation.\n",
      "\n",
      "The code is well-commented. I'll only focus on the interesting stuff here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NgramModel(ModelI):\n",
      "    def __init__(self, n, train, pad_left=True, pad_right=False,\n",
      "                 estimator=None, *estimator_args, **estimator_kwargs):\n",
      "        self._n = n\n",
      "        self._lpad = ('',) * (n - 1) if pad_left else ()\n",
      "        self._rpad = ('',) * (n - 1) if pad_right else ()\n",
      "\n",
      "        if estimator is None:\n",
      "            estimator = _estimator\n",
      "\n",
      "        cfd = ConditionalFreqDist()\n",
      "        self._ngrams = set()\n",
      "\n",
      "\n",
      "        # If given a list of strings instead of a list of lists, create enclosing list\n",
      "        if (train is not None) and isinstance(train[0], compat.string_types):\n",
      "            train = [train]\n",
      "\n",
      "        for sent in train:\n",
      "            for ngram in ngrams(chain(self._lpad, sent, self._rpad), n):\n",
      "                self._ngrams.add(ngram)\n",
      "                context = tuple(ngram[:-1])\n",
      "                token = ngram[-1]\n",
      "                cfd[context].inc(token)\n",
      "\n",
      "        if not estimator_args and not estimator_kwargs:\n",
      "            self._model = ConditionalProbDist(cfd, estimator, len(cfd))\n",
      "        else:\n",
      "            self._model = ConditionalProbDist(cfd, estimator, *estimator_args, **estimator_kwargs)\n",
      "\n",
      "        # recursively construct the lower-order models\n",
      "        if n > 1:\n",
      "            self._backoff = NgramModel(n-1, train, pad_left, pad_right,\n",
      "                                       estimator, *estimator_args, **estimator_kwargs)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see what's going on. \n",
      "\n",
      "The `__init__` function populates two main data structures: `cfd` is the now-familiar `ConditionalFrequencyDistribution`, and `_ngrams` is a set of $n$-length tuples which (we'll see later) is kept for easily testing if a given ngram tuple has been seen in the training set.\n",
      "\n",
      "The estimator is a way of constructing probability distributions from frequency distributions. We'll get into its use later also.\n",
      "\n",
      "This model is designed to work on `list(list(string))`s, which typically means a list of sentences which are lists of tokens or words. This can be beneficial because you don't have to worry about \"walking off\" sentences or being strict with your `[SENTENCE-BREAK]` tokens. \n",
      "\n",
      "If you pass in a `list(string)`, it's wrapped in another list, but not split into sentences. The model makes no assumptions about the meaning of its training data -- this same model could be used for Japanese as well as English, just remember to reverse your lists. Tokenization is left completely to the calling code.\n",
      "\n",
      "The heart of the training process is in this function as well. It's quoted below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for sent in train:\n",
      "    for ngram in ngrams(chain(self._lpad, sent, self._rpad), n):\n",
      "        self._ngrams.add(ngram)\n",
      "        context = tuple(ngram[:-1])\n",
      "        token = ngram[-1]\n",
      "        cfd[context].inc(token)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ah, now this is starting to look familiar, especially the last three lines.\n",
      "\n",
      "Each sentence in the training set is padded up. The `_lpad` and `_rpad`s are optional, but `_lpad` is enabled by default so the first word in the sentence can have a context, even if the context is just a bunch of placeholder strings.\n",
      "\n",
      "Each ngram tuple $(w_{(i-n+1)}, w_{(i-n+2)}, \\dots, w_i)$ is stored in `_ngrams`, and then split into the context (aka the prior) $(w_{(i-n+1)}, w_{(i-n+2)}, \\dots, w_{(i-1)})$ and the current token $w_i$, and put into the `cfd`.\n",
      "\n",
      "There's something important here that's easy to miss: we're outsourcing the actual ngram production to `ngrams()`, because it's useful in other areas. This function is defined in `nltk.util`. Let's look at it too. But before we do, let's think about what we expect this function to be doing. It's being passed a `list(string)` and an $n$, and whatever it outputs is being added straight to the `_ngrams` set. This tells us it must return an iterable over ngram tuples.\n",
      "\n",
      "With that in mind, how would you implement `ngrams()`? I would probably do something like this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _ngrams(words, n):\n",
      "    return (tuple(words[i-(n-1):i+1]) for i in range(n-1, len(words)))\n",
      "\n",
      "list(_ngrams(tokenize(\"This is a sentence\"), 3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "[('THIS', 'IS', 'A'),\n",
        " ('IS', 'A', 'SENTENCE'),\n",
        " ('A', 'SENTENCE', '[SENTENCE-BREAK]')]"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alright, let's compare and contrast with the [NTLK implementation][0], duped below. Once again, I stripped comments.\n",
      "\n",
      "[0]: http://www.nltk.org/_modules/nltk/util.html#ngrams"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ngrams(sequence, n, pad_left=False, pad_right=False, pad_symbol=None):\n",
      "\n",
      "    sequence = iter(sequence)\n",
      "    if pad_left:\n",
      "        sequence = chain((pad_symbol,) * (n-1), sequence)\n",
      "    if pad_right:\n",
      "        sequence = chain(sequence, (pad_symbol,) * (n-1))\n",
      "\n",
      "    history = []\n",
      "    while n > 1:\n",
      "        history.append(next(sequence))\n",
      "        n -= 1\n",
      "    for item in sequence:\n",
      "        history.append(item)\n",
      "        yield tuple(history)\n",
      "        del history[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What the heck?\n",
      "\n",
      "First, since the function is used elsewhere, we have redundant padding functionality. Our NgramModel passes in padded strings already, so that capacity is not used. But what about the rest of it?\n",
      "\n",
      "There's actually something really interesting going on here. First, this is a [generator function](https://wiki.python.org/moin/Generators), which are one of my favorite Python features. If you're not familiar with it, read the wiki, becaues I tried to explain it briefly and it just sounded confusing.\n",
      "\n",
      "So what are we `yield`ing here? Well, the same list -- `history` -- over and over again. Why? Let's see some quadrigrams:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.util import ngrams\n",
      "list(ngrams(tokenize(\"This is a slightly longer sentence\"), 4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[('THIS', 'IS', 'A', 'SLIGHTLY'),\n",
        " ('IS', 'A', 'SLIGHTLY', 'LONGER'),\n",
        " ('A', 'SLIGHTLY', 'LONGER', 'SENTENCE'),\n",
        " ('SLIGHTLY', 'LONGER', 'SENTENCE', '[SENTENCE-BREAK]')]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There's a lot of repetition there. The ngram is something like a \"moving window\" into the text. It makes sense to preserve the elements that are repeated between grams, dropping from the beginning and appending to the end.\n",
      "\n",
      "So this function first produces a list of the first $n$ words, and then for each next word in the sequence, it `del`s the \"oldest\" word in `history` and appends the new one to produce the next ngram.\n",
      "\n",
      "This might involve less memory allocation since it only uses a single list. But it does still have to create a new tuple for each ngram (you can only hash immutable data types), so I'm not sure if the savings are significant. Let's run a test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "token_poetry = tokenize(gutenberg.raw(\"austen-emma.txt\"))\n",
      "%timeit -n 5 list(ngrams(token_poetry, 4))\n",
      "%memit list(ngrams(token_poetry, 4))\n",
      "%timeit -n 5 list(_ngrams(token_poetry, 4))\n",
      "%memit list(_ngrams(token_poetry, 4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5 loops, best of 3: 115 ms per loop\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "peak memory: 79.31 MiB, increment: 11.40 MiB\n",
        "5 loops, best of 3: 151 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "peak memory: 79.29 MiB, increment: 11.64 MiB\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Seems using an inplace list is a little faster than the naive approach, but not much. \n",
      "\n",
      "Note that generator functions, like all iterators, are lazily evaluated, so we force evaluation by pulling the iterable into an actual list. (Also note that `_ngrams` also returns an iterator because it uses a [generator expression](https://docs.python.org/3/reference/expressions.html#generator-expressions).)\n",
      "\n",
      "Anyway. Enough performance.\n",
      "\n",
      "Now we know how `ngram()` works, let's carry on with `NgramModel.__init__()`. The next lines are:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not estimator_args and not estimator_kwargs:\n",
      "    self._model = ConditionalProbDist(cfd, estimator, len(cfd))\n",
      "else:\n",
      "    self._model = ConditionalProbDist(cfd, estimator, *estimator_args, **estimator_kwargs)\n",
      "\n",
      "# recursively construct the lower-order models\n",
      "if n > 1:\n",
      "    self._backoff = NgramModel(n-1, train, pad_left, pad_right,\n",
      "                               estimator, *estimator_args, **estimator_kwargs)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, once again we have some `estimator` business. \n",
      "\n",
      "Then, we are actually doing a *recursive construction* of all n-gram models with a lower $n$, and storing it in `_backoff`. This seems like a potential performance concern! Why do we need so many models?\n",
      "\n",
      "Surprisingly, this is a pretty common tactic. As $n$ grows, the $n$-gram model retains more and more context. This can be beneficial -- certainly, it gets us closer to the truth, which is that each word is contextualized by the *entire* preceding text and arguably the succeeding text as well. But it can also be dangerous: as we add more and more context, the *incidence* of a specific context decreases. For instance, consider quintigrams: it's pretty unlikely that any but the biggest corpus is going to contain every quintigrams you throw at it, and sexigrams are even worse.\n",
      "\n",
      "So what do you do if you can't match the provided context exactly? Just start throwing context out until you find a match -- which is what the $n-1$ model in `_backoff` is for.\n",
      "\n",
      "Concretely: what if you are trying to find the probability of \"steamed\" following \"I get all\" $P(\\text{steamed}|\\text{I get all})$, but your corpus doesn't contain nursery rhymes? You can \"back off\" to looking for $P(\\text{steamed}|\\text{get all})$. And if you can't find that, back off to $P(\\text{steamed}|\\text{all})$, which maybe exists in your corpus because it was found in \"They all steamed vegetables every morning\" or \"her clothes were all steamed and pressed\". This \"backing off\" the context can give you an answer, but at the cost of discarding information.\n",
      "\n",
      "It's possible to implement backoffs without using backoff models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}